{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cfa27ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f6273e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'findspark' from 'C:\\\\Users\\\\W10\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\lib\\\\site-packages\\\\findspark.py'>\n"
     ]
    }
   ],
   "source": [
    "print(findspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57ecde12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b1c0e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('hai').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1414247b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read \\\n",
    "            .option('header',True) \\\n",
    "            .option('inferSchema',True) \\\n",
    "            .csv(\"C:/Users/W10/Desktop/sparkTxt/annual-csv.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "56704b6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+-------+----------------------+\n",
      "|Year|Industry_aggregation_NZSIOC|Industry_code_NZSIOC|Industry_name_NZSIOC|             Units|Variable_code|       Variable_name|   Variable_category|  Value|Industry_code_ANZSIC06|\n",
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+-------+----------------------+\n",
      "|2020|                    Level 1|               99999|      All industries|Dollars (millions)|          H01|        Total income|Financial perform...|733,258|  ANZSIC06 division...|\n",
      "|2020|                    Level 1|               99999|      All industries|Dollars (millions)|          H04|Sales, government...|Financial perform...|660,630|  ANZSIC06 division...|\n",
      "|2020|                    Level 1|               99999|      All industries|Dollars (millions)|          H05|Interest, dividen...|Financial perform...| 54,342|  ANZSIC06 division...|\n",
      "|2020|                    Level 1|               99999|      All industries|Dollars (millions)|          H07|Non-operating income|Financial perform...| 18,285|  ANZSIC06 division...|\n",
      "|2020|                    Level 1|               99999|      All industries|Dollars (millions)|          H08|   Total expenditure|Financial perform...|654,872|  ANZSIC06 division...|\n",
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+-------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5,truncate=True, vertical=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4eaab2ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Year: string (nullable = true)\n",
      " |-- Industry_aggregation_NZSIOC: string (nullable = true)\n",
      " |-- Industry_code_NZSIOC: string (nullable = true)\n",
      " |-- Industry_name_NZSIOC: string (nullable = true)\n",
      " |-- Units: string (nullable = true)\n",
      " |-- Variable_code: string (nullable = true)\n",
      " |-- Variable_name: string (nullable = true)\n",
      " |-- Variable_category: string (nullable = true)\n",
      " |-- Value: string (nullable = true)\n",
      " |-- Industry_code_ANZSIC06: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64f4d950",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3efd26e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "[ \\\n",
    "    StructField(\"Year\", IntegerType(), True), \\\n",
    "    StructField(\"Industry_aggregation_NZSIOC\", StringType(), True), \\\n",
    "    StructField(\"chaddi baniyan\", StringType(), True), \\\n",
    "    StructField(\"Industry_name_NZSIOC\", StringType(), True), \\\n",
    "    StructField(\"Units\", StringType(), True), \\\n",
    "    StructField(\"Variable_code\", StringType(), True), \\\n",
    "    StructField(\"Variable_name\", StringType(), True), \\\n",
    "    StructField(\"Variable_category\", StringType(), True), \\\n",
    "    StructField(\"Value\", StringType(), True), \\\n",
    "    StructField(\"Industry_code_ANZSIC06\", StringType(), True)\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4cc672d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = spark.read.csv('C:/Users/W10/Desktop/sparkTxt/annual-csv.csv', schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e40bc15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+-------------+--------------------+-------+----------------------+\n",
      "|Year|Industry_aggregation_NZSIOC|      chaddi baniyan|Industry_name_NZSIOC|             Units|Variable_code|Variable_name|   Variable_category|  Value|Industry_code_ANZSIC06|\n",
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+-------------+--------------------+-------+----------------------+\n",
      "|null|       Industry_aggregat...|Industry_code_NZSIOC|Industry_name_NZSIOC|             Units|Variable_code|Variable_name|   Variable_category|  Value|  Industry_code_ANZ...|\n",
      "|2020|                    Level 1|               99999|      All industries|Dollars (millions)|          H01| Total income|Financial perform...|733,258|  ANZSIC06 division...|\n",
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+-------------+--------------------+-------+----------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5cfe06cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.write.format('csv').mode('Overwrite').option('maxRecordsPerFile',500).option('path','output').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90f76ade",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37082"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfc82f27",
   "metadata": {},
   "outputs": [],
   "source": [
    " df2.registerTempTable(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cf311d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+-------+----------------------+\n",
      "|Year|Industry_aggregation_NZSIOC|      chaddi baniyan|Industry_name_NZSIOC|             Units|Variable_code|       Variable_name|   Variable_category|  Value|Industry_code_ANZSIC06|\n",
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+-------+----------------------+\n",
      "|null|       Industry_aggregat...|Industry_code_NZSIOC|Industry_name_NZSIOC|             Units|Variable_code|       Variable_name|   Variable_category|  Value|  Industry_code_ANZ...|\n",
      "|2020|                    Level 1|               99999|      All industries|Dollars (millions)|          H01|        Total income|Financial perform...|733,258|  ANZSIC06 division...|\n",
      "|2020|                    Level 1|               99999|      All industries|Dollars (millions)|          H04|Sales, government...|Financial perform...|660,630|  ANZSIC06 division...|\n",
      "|2020|                    Level 1|               99999|      All industries|Dollars (millions)|          H05|Interest, dividen...|Financial perform...| 54,342|  ANZSIC06 division...|\n",
      "|2020|                    Level 1|               99999|      All industries|Dollars (millions)|          H07|Non-operating income|Financial perform...| 18,285|  ANZSIC06 division...|\n",
      "+----+---------------------------+--------------------+--------------------+------------------+-------------+--------------------+--------------------+-------+----------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+---------+\n",
      "|namespace|\n",
      "+---------+\n",
      "|  default|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from dfTable').show(5)\n",
    "spark.sql('show databases').show()\n",
    "# spark.sql('create database testing')\n",
    "df2.createOrReplaceTempView('permTabledf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e81a2077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "\n",
    "jsondata = requests.request('GET','https://jsonplaceholder.typicode.com/todos/')\n",
    "# print(jsondata.json())\n",
    "j = (jsondata.json())\n",
    "print(type(j))\n",
    "\n",
    "file = open('C:/Users/W10/Desktop/sparkTxt/testing.json','a')\n",
    "for r in j:\n",
    "    file.write(str(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde9ce12",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "848f040d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([\"Project Gutenberg’s\",\n",
    "        \"Alice’s Adventures in Wonderland\",\n",
    "        \"Project Gutenberg’s\",\n",
    "        \"Adventures in Wonderland\",\n",
    "        \"Project Gutenberg’s\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45b6e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5d1ad57c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Project Gutenberg’s',\n",
       " 'Alice’s Adventures in Wonderland',\n",
       " 'Project Gutenberg’s',\n",
       " 'Adventures in Wonderland',\n",
       " 'Project Gutenberg’s']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54d42011",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd2 = rdd.flatMap(lambda x : x.split(\" \")).map(lambda x : (x,1)).reduceByKey(lambda x,y : x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a5fe969",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Gutenberg’s', 3),\n",
       " ('Alice’s', 1),\n",
       " ('in', 2),\n",
       " ('Adventures', 2),\n",
       " ('Wonderland', 2),\n",
       " ('Project', 3)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8d5e1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "datardd = spark.sparkContext.textFile('C:/Users/W10/Desktop/sparkTxt/spark.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21a9354b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['spark spark spark spark spark spark spark spark spark spark',\n",
       " 'spark spark spark spark spark spark',\n",
       " 'spark spark spark spark spark spark spark',\n",
       " 'spark',\n",
       " 'spark spark spark',\n",
       " 'spark spark spark spark']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datardd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f39ccc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'spark'],\n",
       " ['spark', 'spark', 'spark', 'spark', 'spark', 'spark'],\n",
       " ['spark', 'spark', 'spark', 'spark', 'spark', 'spark', 'spark'],\n",
       " ['spark'],\n",
       " ['spark', 'spark', 'spark'],\n",
       " ['spark', 'spark', 'spark', 'spark']]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2 = datardd.map(lambda x : x.split(' '))\n",
    "r2.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d35db44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "listt=r2.map(lambda x : len(x)).collect()\n",
    "maxele=max(listt)\n",
    "print(maxele)\n",
    "\n",
    "# def x= ['spark', 'spark', 'spark', 'spark', 'spark', 'spark']\n",
    "\n",
    "def addElements(x):\n",
    "    for i in range(maxele):\n",
    "        if len(x) != maxele:\n",
    "            x.append('nodata')\n",
    "        else:\n",
    "            break\n",
    "    return x\n",
    "            \n",
    "r3 = r2.map(lambda x : addElements(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d9ea837",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'spark'],\n",
       " ['spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'nodata',\n",
       "  'nodata',\n",
       "  'nodata',\n",
       "  'nodata'],\n",
       " ['spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'nodata',\n",
       "  'nodata',\n",
       "  'nodata'],\n",
       " ['spark',\n",
       "  'nodata',\n",
       "  'nodata',\n",
       "  'nodata',\n",
       "  'nodata',\n",
       "  'nodata',\n",
       "  'nodata',\n",
       "  'nodata',\n",
       "  'nodata',\n",
       "  'nodata'],\n",
       " ['spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'nodata',\n",
       "  'nodata',\n",
       "  'nodata',\n",
       "  'nodata',\n",
       "  'nodata',\n",
       "  'nodata',\n",
       "  'nodata'],\n",
       " ['spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'spark',\n",
       "  'nodata',\n",
       "  'nodata',\n",
       "  'nodata',\n",
       "  'nodata',\n",
       "  'nodata',\n",
       "  'nodata']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8d9b5ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=r3.toDF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cde0a075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+------+------+------+------+------+------+------+------+\n",
      "|   _1|    _2|    _3|    _4|    _5|    _6|    _7|    _8|    _9|   _10|\n",
      "+-----+------+------+------+------+------+------+------+------+------+\n",
      "|spark| spark| spark| spark| spark| spark| spark| spark| spark| spark|\n",
      "|spark| spark| spark| spark| spark| spark|nodata|nodata|nodata|nodata|\n",
      "|spark| spark| spark| spark| spark| spark| spark|nodata|nodata|nodata|\n",
      "|spark|nodata|nodata|nodata|nodata|nodata|nodata|nodata|nodata|nodata|\n",
      "|spark| spark| spark|nodata|nodata|nodata|nodata|nodata|nodata|nodata|\n",
      "|spark| spark| spark| spark|nodata|nodata|nodata|nodata|nodata|nodata|\n",
      "+-----+------+------+------+------+------+------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "442a3068",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew = spark.read \\\n",
    "                .option('header',True) \\\n",
    "                .csv('C:/Users/W10/Desktop/sparkTxt/rating2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb347e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+---------+\n",
      "|userId|movieId|rating| timestamp|ratingStr|\n",
      "+------+-------+------+----------+---------+\n",
      "|     1|     39|   2.6|1260759144|      one|\n",
      "|     1|     38|   2.7|      null|      two|\n",
      "|     1|   null|   2.7|1260759140|    three|\n",
      "|  null|     36|   3.8|1260759149|     null|\n",
      "|     1|     35|   4.9|1260759148|     five|\n",
      "|     1|   null|     5|      null|     null|\n",
      "|  null|     33|   3.1|1260759146|    seven|\n",
      "|     1|     32|  null|1260759145|    eight|\n",
      "+------+-------+------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfnew.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c63fb8b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "dfnew = dfnew.na.drop()\n",
    "# dfnew.show()\n",
    "\n",
    "def f2(x):\n",
    "    return x*x\n",
    "\n",
    "spark.udf.register('myfunc',f2,IntegerType())\n",
    "myfunc = udf(f2,IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55cf08d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def convertCase(str):\n",
    "    resStr=\"\"\n",
    "    arr = str.split(\" \")\n",
    "    for x in arr:\n",
    "       resStr= resStr + x[0:1].upper() + x[1:len(x)] + \" \"\n",
    "    return resStr\n",
    "\n",
    "convertUDF = udf(lambda z: convertCase(z),StringType())\n",
    "convertUDF = udf(lambda z: convertCase(z)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f6fb6b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfnew = dfnew.withColumn('katainewtimestamp',current_date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0d29e541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+----------+---------+-----------------+\n",
      "|userId|movieId|rating| timestamp|ratingStr|katainewtimestamp|\n",
      "+------+-------+------+----------+---------+-----------------+\n",
      "|     1|     39|   2.6|1260759144|      one|       2022-09-26|\n",
      "|     1|     35|   4.9|1260759148|     five|       2022-09-26|\n",
      "+------+-------+------+----------+---------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfnew.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ee2af589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'(2) PythonRDD[42] at collect at C:\\\\Users\\\\W10\\\\AppData\\\\Local\\\\Temp\\\\ipykernel_19748\\\\1450628862.py:1 []\\n |  C:/Users/W10/Desktop/sparkTxt/spark.txt MapPartitionsRDD[39] at textFile at NativeMethodAccessorImpl.java:0 []\\n |  C:/Users/W10/Desktop/sparkTxt/spark.txt HadoopRDD[38] at textFile at NativeMethodAccessorImpl.java:0 []'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r3.toDebugString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eda2238e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [userId#342, movieId#343, rating#344, timestamp#345, ratingStr#346, 19261 AS katainewtimestamp#383]\n",
      "+- *(1) Filter AtLeastNNulls(n, userId#342,movieId#343,rating#344,timestamp#345,ratingStr#346)\n",
      "   +- FileScan csv [userId#342,movieId#343,rating#344,timestamp#345,ratingStr#346] Batched: false, DataFilters: [AtLeastNNulls(n, userId#342,movieId#343,rating#344,timestamp#345,ratingStr#346)], Format: CSV, Location: InMemoryFileIndex[file:/C:/Users/W10/Desktop/sparkTxt/rating2.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<userId:string,movieId:string,rating:string,timestamp:string,ratingStr:string>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfnew.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9d77196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "42dc57b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [38]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dfstream \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minferSchema\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:/Users/W10/Desktop/sparkTxt/annual-csv.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\streaming.py:766\u001b[0m, in \u001b[0;36mDataStreamReader.csv\u001b[1;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup)\u001b[0m\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[0;32m    754\u001b[0m     schema\u001b[38;5;241m=\u001b[39mschema, sep\u001b[38;5;241m=\u001b[39msep, encoding\u001b[38;5;241m=\u001b[39mencoding, quote\u001b[38;5;241m=\u001b[39mquote, escape\u001b[38;5;241m=\u001b[39mescape, comment\u001b[38;5;241m=\u001b[39mcomment,\n\u001b[0;32m    755\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader, inferSchema\u001b[38;5;241m=\u001b[39minferSchema, ignoreLeadingWhiteSpace\u001b[38;5;241m=\u001b[39mignoreLeadingWhiteSpace,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    763\u001b[0m     emptyValue\u001b[38;5;241m=\u001b[39memptyValue, locale\u001b[38;5;241m=\u001b[39mlocale, lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[0;32m    764\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter, recursiveFileLookup\u001b[38;5;241m=\u001b[39mrecursiveFileLookup)\n\u001b[0;32m    765\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, basestring):\n\u001b[1;32m--> 766\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    767\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath can be only a single string\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\spark-3.0.3-bin-hadoop2.7\\python\\lib\\py4j-0.10.9-src.zip\\py4j\\java_gateway.py:1304\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1308\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mD:\\spark-3.0.3-bin-hadoop2.7\\python\\pyspark\\sql\\utils.py:134\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    130\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[43mraise_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconverted\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32m<string>:3\u001b[0m, in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: Schema must be specified when creating a streaming source DataFrame. If some files already exist in the directory, then depending on the file format you may be able to create a static DataFrame on that directory with 'spark.read.load(directory)' and infer schema from it."
     ]
    }
   ],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6331fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
